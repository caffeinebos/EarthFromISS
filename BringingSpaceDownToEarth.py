##############################################
#                                            #
#            \\\\        \\\\                #
#            \\\\  -o-   \\\\                #
#      o0000000000000000000000000o           #
#            ||||   0     ||||               #
#           ||||   -0-   ||||                #
#                                            #
#                                            #
#          _..._                             #
#        .'     '.      _                    #
#       /    .-""-\   _/ \                   #
#    .-|   /:.   |  |   |                    #
#     |  \  |:.   /.-'-./                    #
#     | .-'-;:__.'    =/                     #
#     .'=  *=|NASA _.='                      #
#    /   _.  |    ;                          #
#   ;-.-'|    \   |                          #
#  /   | \    _\  _\                         #
#  \__/'._;.  ==' ==\                        #
#           \    \   |                       #
#           /    /   /                       #
#           /-._/-._/                        #
#   jgs     \   `\  \                        #
#            `-._/._/                        #
#                                            #
# Web Scraping program: High Resoluation     #
# Photography from the ISS Astronauts,       #
# Automatically downlaoded. Bringing space,  #
# down to earth.                             #
#                                            #
# Astronaut ASCII Art:                       #
# https://www.asciiart.eu/space/astronauts   #
#                                            #
# Written by:                                #
# caffeine_bos - 03/2020                     #
##############################################

#The random, os, and requests modules are required to run this program.
#Optionally, this program can use the webbrowser module.
#If you don't have random, or requests installed they can be installed with pip.
#What's pip? You can find more information and installation files here: https://pypi.org/project/pip/
#If you have pip, type "pip install random, requests" into the command line.

#You can view the URLs as they're being generated by un-commenting (Deleting the '#') the 'webbrowser' module below, and its respective command in the loop.
import random, os, requests #webbrowser

root_directory = os.path.dirname(os.path.abspath(__file__))
directory = os.path.join(root_directory, "images")

#Set the directory to where the files will be saved to
if not os.path.exists(directory):
    os.mkdir(directory)

#Create the variable for the text file to save the Valid URLs, to avoid duplicates
validURLs = os.path.join(root_directory, "ValidURLs.txt")

#Iterate 20 times
for URL in range(1,20):
    #Generate a random mission number between 10 and 62
    mission = random.randint(10,62)
    #Generate a random photo number between 1 and 250,000
    photonum = random.randint(1,250000)
    #Define the format of the URL to be generated.
    Format = "https://eol.jsc.nasa.gov/DatabaseImages/ESC/Large/ISS" + "0" + str(mission) + "/ISS" + "0" + str(mission) + "-E-" + str(photonum) + ".JPG"
    #Request the download
    response = requests.get(Format)
    #Check to see if it was a 404 error, or successful.
    URLfalse = response.status_code

    #If you want to view the URL's as they are generated, uncomment the next line. This is not recommended, as it takes up quite a bit of system memory.
    #webbrowser.open(Format)

    #Open the text file
    URLlist = open(validURLs)

    #If the text file already contains the URL that was generated, close it and loop again.
    if Format in validURLs:
        URLlist.close()

    #If not in the text file, and it was a successful request
    elif Format not in validURLs and URLfalse != 404:
        #Define how the file name will be saved, as "/, :" are invalid characters, and will confuse the path.
        fname = Format[57:]
        #Open the file to save it with the name.
        image = open(os.path.join(directory, fname), "wb")
        #Write the file in chunks up to 25mb (None of the files will be this large)
        for chunk in response.iter_content(25000000):
            image.write(chunk)
        #Close the download request
        image.close()
        #Open the text file in append mode
        URLlist = open(validURLs,"a")
        #Write the URL and add a new line
        URLlist.write(Format + "\n")
        #Close the text file
        URLlist.close()

    #Print out a list of the URL's generated to see.
    print(Format)
